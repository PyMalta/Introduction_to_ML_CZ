{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:35.863348Z",
     "start_time": "2019-09-30T19:21:35.860696Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:35.868499Z",
     "start_time": "2019-09-30T19:21:35.865508Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 18,\n",
    "                            'lines.linewidth' : 3,\n",
    "                           'figure.figsize' : [15, 5],\n",
    "                           'lines.markersize': 10})\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous session\n",
    "We had a teaser of what to expect when doing Machine Learning. By now you should know:\n",
    "1. How to use `scikit-learn`\n",
    "    - **predictors**: `fit(X)`, `predict(X)`\n",
    "    - **transformers**: `fit(X)`, `transform(X)`, `fit_transform(X)`\n",
    "1. What metrics to use for Supervised classification learning    \n",
    "1. Be able to build your first `Decision Tree` binary classifier using the *DieTanic* dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "We have defined machine learning as the process of optimizing the cost function of the problem by tweaking the parameters of a model. We have defined a model as some map that uniquely maps input (features) to output (labels or target values). We've left this definition very vague, because there are many popular models used for machine learning. We have already looked at decision trees and now we will investigate how to choose the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:35.891300Z",
     "start_time": "2019-09-30T19:21:35.870564Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic = pd.read_csv('./00_data/titanic_X.csv')\n",
    "df_titanic_target = pd.read_csv('./00_data/titanic_y.csv', squeeze=True,header=None)\n",
    "\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:35.897262Z",
     "start_time": "2019-09-30T19:21:35.893091Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:35.902517Z",
     "start_time": "2019-09-30T19:21:35.898828Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(df_titanic)\n",
    "y = np.array(df_titanic_target)\n",
    "\n",
    "df_titanic['Survived'] = df_titanic_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:\n",
    "1. Load **./00_data/dietanic.csv**\n",
    "1. Identify the feature matrix `X` and the labels `y`.\n",
    "2. Create training and testing dataset: `X_train, y_train, X_test, y_test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Recap*: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:36.339826Z",
     "start_time": "2019-09-30T19:21:35.904828Z"
    }
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# train decision tree\n",
    "dtc = DecisionTreeClassifier(max_depth=2,)\n",
    "y_pred = dtc.fit(X,y).predict(X)\n",
    "\n",
    "# visual tree\n",
    "graphviz.Source(export_graphviz(dtc, \n",
    "                                out_file=None,\n",
    "                                feature_names=np.array(df_titanic.drop('Survived', axis=1).columns.tolist()),\n",
    "                                class_names=np.array(['0','1'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By allowing for more branching, does this make our model better or worse?¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:36.347200Z",
     "start_time": "2019-09-30T19:21:36.342155Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('f1-Score : {}'.format(f1_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:36.655593Z",
     "start_time": "2019-09-30T19:21:36.348854Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depths = range(1,40)\n",
    "f1Score = []\n",
    "for md in max_depths:\n",
    "    dtc = DecisionTreeClassifier(max_depth=md)\n",
    "    y_pred = dtc.fit(X,y).predict(X)\n",
    "    f1Score.append(f1_score(y, y_pred))\n",
    "    \n",
    "plt.plot(max_depths, f1Score, label = 'f1-Score')\n",
    "plt.xlabel('Maximum Depth Hyperparameter')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach a conflict: the model keeps getting better and better the deeper we go with **max_depth** and beyond 18, we are suffering from _overfitting_. The model does not seem to improve and at 24 max_depth, the precision actually decreases as this now starts to follow noise in the data. To detect overfitting, we need to see how our model generalizes to new data. We can do this artificially by withholding part of our data set during the training step, and then using it to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:37.084377Z",
     "start_time": "2019-09-30T19:21:36.657381Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_titanic.drop(['Survived'], axis=1),\n",
    "                                                   df_titanic['Survived'],\n",
    "                                                   test_size = 0.1)\n",
    "f1Score_test = []\n",
    "for md in max_depths:\n",
    "    dtc = DecisionTreeClassifier(max_depth=md)\n",
    "    y_pred = dtc.fit(X_train,y_train).predict(X_test)\n",
    "    f1Score_test.append(f1_score(y_test, y_pred))\n",
    "    \n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(max_depths, f1Score, label = 'f1-Score')\n",
    "plt.plot(max_depths, f1Score_test, '--', label = 'f1-Score test')\n",
    "plt.xlabel('Maximum Depth Hyperparameter')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T13:41:50.244487Z",
     "start_time": "2019-09-30T13:41:50.239547Z"
    }
   },
   "source": [
    "The testing error confirms our suspicion. As the model becomes more complex, it improves up to a point, and then it loses generalizability. **The testing metrics actually start decreasing!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T13:42:01.415176Z",
     "start_time": "2019-09-30T13:42:01.410979Z"
    }
   },
   "source": [
    "## Exercise 2:\n",
    "1. Train a decision tree classifier with hyperparameter **max_depth = 3** and evaluate the f1-score\n",
    "1. Train a decision tree classifier with hyperparameter **max_depth = 15** and evaluate the f1-score\n",
    "1. Train a decision tree classifier with hyperparameter **max_depth = 40** and evaluate the f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters: tuning and cross-validation\n",
    "\n",
    "Which are the optimal hyperparameters for a decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:37.194516Z",
     "start_time": "2019-09-30T19:21:37.086813Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth = [5,10,15,20,25,30]\n",
    "min_samples_split = [25, 50, 75, 100, 125, 150, 175, 200]\n",
    "import itertools\n",
    "\n",
    "combns = [ii  for ii in itertools.product(max_depth, min_samples_split)] \n",
    "\n",
    "combns_x = [x[0] for x in combns]\n",
    "combns_y = [x[1] for x in combns]\n",
    "plt.scatter(combns_x, combns_y)\n",
    "plt.xlabel('max_depth')\n",
    "plt.xticks(max_depth)\n",
    "plt.ylabel('min_samples_split')\n",
    "plt.yticks(min_samples_split)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we defined our decision tree estimator, we chose how many layers the tree would have using the `max_depth` keyword. When we initiate an estimator, we can pass keyword arguments that will dictate its structure. The decision tree classifier accepts [13 different keyword arguments](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). These arguments are called _hyperparameters_. This is in contrast to _parameters_, which are the numbers that our model uses to predict labels based on features. _Parameters_ are optimized during training. _Hyperparameters_ are decided before training and dictate the model's structure. Basically all models have hyperparameters. Even a simple linear regressor has a hyperparameter `fit_intercept`.\n",
    "\n",
    "Since changing hyperparameters changes the structure of the model, we should think of choosing hyperparameters as part of model selection. `Scikit-learn` provides a useful tool for comparing different hyperparameter values, `GridSearchCV`. There are two ideas behind `GridSearchCV`: first we will split up the data into a training and validation set (using a method called [k-folds](http://scikit-learn.org/stable/modules/cross_validation.html#k-fold)) and then we train and evaluate models with different hyperparameters selected from a grid of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:37.645059Z",
     "start_time": "2019-09-30T19:21:37.196119Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "grid = GridSearchCV(dtc,\n",
    "                   {'max_depth' : range(1,15),\n",
    "                   'min_samples_split': range(10, 110, 10)},\n",
    "                    cv=5,\n",
    "                    n_jobs=5,\n",
    "                    scoring='f1',\n",
    "                   verbose=1)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:37.650210Z",
     "start_time": "2019-09-30T19:21:37.646432Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:37.654804Z",
     "start_time": "2019-09-30T19:21:37.651476Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:37.661123Z",
     "start_time": "2019-09-30T19:21:37.656751Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:37.666050Z",
     "start_time": "2019-09-30T19:21:37.662590Z"
    }
   },
   "outputs": [],
   "source": [
    "len(range(1,15)) * len(range(10, 110, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "1. Find the optimal decision tree classifier using a parameter search of:\n",
    "    - **max_depth**: 5 to 25, increments of 5\n",
    "    - **min_samples_split**: \\[2, 6, 10 \\]\n",
    "    - **max_features**: *see documentation for this**\n",
    "    - **random_state**: set seed to be 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Caution*: Compounded growth of hyperparameter grid search\n",
    "\n",
    "Caution is advised when doing hyperparameter tuning using GridSearchCV. Each additional hyperparameter added to the grid search compoundedly grows the number of parameters to search. For example, consider 3 hyperparameters for decision tree, with respective search arrays\n",
    "\n",
    "|Hyperparameter| Search Array | Number of hyperparameters|\n",
    "|---|---|---|\n",
    "|max_depth |*\\[5, 15, 25, 30, 45, 55, 75, 100, 125, 150\\]* | 10 |\n",
    "|max_features |*\\['auto','sqrt','log'\\]* | 3 |\n",
    "|min_samples_leaf| *\\[25, 50, 75, 100, 125, 150, 175, 200 \\]* |8|\n",
    "\n",
    "Assume that we are doing 5-fold cv on the data,\n",
    "\n",
    "Total number of hyperparameters to be searched are: $ 10*3*8*5 = 1200 $\n",
    "\n",
    "If we add 2 more hyperparamters to *max_depth*, we now need to search through $6*3*8*5 = 1440$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:37.849627Z",
     "start_time": "2019-09-30T19:21:37.667567Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot3DSearchGrid(angle):\n",
    "    xx = [5, 15, 25, 30, 45, 55, 75, 100, 125, 150]\n",
    "    yy = ['auto','sqrt','log']\n",
    "    zz = [25, 50, 75, 100, 125, 150, 175, 200 ]\n",
    "\n",
    "    X, Y, Z = np.mgrid[range(len(xx)), range(len(yy)), range(25,225,25)]\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scat = ax.scatter(X, Y, Z, c=Z.flatten(), s=100)\n",
    "    plt.xticks(np.arange(len(xx)), xx)\n",
    "    ax.set_xlabel('max_depth', labelpad=30)\n",
    "    plt.yticks(np.arange(len(yy)), yy)\n",
    "    ax.set_ylabel('min_samples_leaf', labelpad=30)\n",
    "\n",
    "    ax.set_zlabel('max_features', labelpad=30)\n",
    "    ax.view_init(30, angle)\n",
    "    \n",
    "axes_angle = IntSlider(min=0, max=180, step=15, description='Angle') \n",
    "interact(plot3DSearchGrid, angle=axes_angle);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further look at Decision Trees and other Tree Based Models\n",
    "## Random forests\n",
    "<img src =\"00_pics/random_forest.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T12:45:27.381413Z",
     "start_time": "2019-08-22T12:45:27.372285Z"
    }
   },
   "source": [
    "The performance of a single decision tree will be limited. Instead of relying on one tree, a better approach is to aggregate the predictions of multiple tree. On average, aggregation will perform better than a single predictor. You can envision the aggregation as mimicking the idea of \"wisdom of the crowd\". We call a tree based model that aggregates the predictions of multiple trees a __random forest__.\n",
    "\n",
    "In order for a random forest to be effective, the model needs a diverse collection of trees. There should be variations in the chosen thresholds for splitting and the number of nodes and branches. There is no point in aggregating the predicted results if all the trees are nearly identical and produce the same result. There is no \"wisdom of the crowd\" if everyone thinks alike. To achieve a diverse set of trees, we need to:\n",
    "\n",
    "1. Train each tree in the forest using a different training set.\n",
    "1. Only consider a subset of features when deciding how to split the nodes.\n",
    "\n",
    "For the first point, ideally we would generate a new training set for each tree. However, often times it's too difficult or expensive to collect more data; we have to make due with what we have. Bootstrapping is a general statistical technique to generate \"new\" data sets with a single set by random sampling with _replacement_. Sampling with replacement allows for a data point to be sampled more than once.\n",
    "\n",
    "Typically, when training the standard decision tree model, the algorithm will consider all features in deciding the node split. Considering only a subset of your features ensures that your trees do not resemble each other. If the algorithm had considered all features, a dominant feature would be continuously chosen for node splits.\n",
    "\n",
    "The hyperparameters available for random forests include those of decision tress with some additions.\n",
    "\n",
    "|Hyperparameter | Description |\n",
    "|---|---|\n",
    "|n_estimators|The number of trees in the forest|\n",
    "|n_jobs|The number of jobs to run in parallel when fitting and predicting|\n",
    "|warm_start|If set to `True`, reuse the trained tree from a prior fitting and just train the additional trees|\n",
    "\n",
    "\n",
    "Since the random forest is based on idea of bootstrapping and aggregating the results, it is referred to as a *bagging* ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a plot of the test set f1-Score as a function of the size of the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:38.659156Z",
     "start_time": "2019-09-30T19:21:37.851144Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def rfc_f1Score(max_features,n_max):\n",
    "    rfc = RandomForestClassifier(max_features=max_features,\n",
    "                                max_depth=8,\n",
    "                                n_estimators=1,\n",
    "                                warm_start=True,\n",
    "                                random_state=0)\n",
    "    \n",
    "    f1Score = np.zeros(n_max)\n",
    "\n",
    "    for n in range(1, n_max):\n",
    "        rfc.set_params(n_estimators=n)\n",
    "        rfc.fit(X_train, y_train)\n",
    "        f1Score[n-1] = f1_score(y_test, rfc.predict(X_test))\n",
    "    \n",
    "    return f1Score\n",
    "\n",
    "f1 = rfc_f1Score(max_features='sqrt', n_max=100)\n",
    "plt.plot(f1[:-1], label = 'sqrt')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('f1-Score');\n",
    "# plt.legend();    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several takeaways from the results plotted above.\n",
    "\n",
    "1. There is sharp increase in metric performance when initially growing the forest.\n",
    "1. In general, as the number trees increase, performance increases.\n",
    "1. You _can_ overfit with a large number of trees but the model is robust to overfitting with the number of trees\n",
    "1. Note, the model was not tuned for hyperparameters like `max_depth` and `min_samples_split`.\n",
    "\n",
    "The initial increase in f1-Score can be attributed to a large increase in diverse trees when the forest is small. In other words, the additional trees will be very different from the current trees simply because the forest is small. The increase in tree diversity drives predictive power. As the forest grows, newer trees will not be significantly different from the current pool of trees as bootstrapping is no longer producing substantially diverse training sets to create very different looking trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "1. Fit a Random Forest Classifier and evaluate the f1-score\n",
    "2. Compare to the default Decision Tree result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees\n",
    "\n",
    "An extremely randomized trees is a variation of the random forest model that injects additional randomness. As with a random forest, a random subset of the features are selected to determine which one to use for a node split. However, instead of considering the optimal split point _for each_ selected feature, a candidate for the split for each feature is chosen at _random_. From these randomly chosen values, the best is chosen to perform the split. The extra randomness serves two folds: \n",
    "1. it helps reduce the model's variance, and \n",
    "2. leads to faster training times. \n",
    "\n",
    "In `scikit-learn`, the extremely randomized tree model is provided by `ensemble.ExtraTreesClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Trees\n",
    "<img src=\"00_pics/gradient_boosting.png\" >\n",
    "\n",
    "Gradient boosting trees are another ensemble model; it is collection of tree models arranged in a sequence. Here, the model is built stage-wise; each additional tree aims to correct the previously built model's predictions. A model with $M$ trees is equal to \n",
    "\n",
    "$$ f_M(x_j) =  \\sum^M_m \\gamma_m h_m(x_j), $$\n",
    "\n",
    "where $h_m$ is a **weak learner** decision tree, a \"stump\" model with low depth that performs poorly on its own. The term **boosting** refers to the algorithm's ability to combine multiple weak learners to form a strong learner. $\\gamma_m$ is a factor that scales the contribution of a tree to the overall model. How are $h_m$ and $\\gamma_m$ chosen? The model is usually initialized with $h_0$ being equal to the mean of the training labels for regression or the majority class for classification. We also need to choose a loss function $L(y, f_m)$. For example, if the loss function is squared error, then\n",
    "\n",
    "$$ L_{SE}(y_j, f_m(x_j)) = (y_j-f_m(x_j))^2. $$ \n",
    "\n",
    "The steps for building our model with $M$ trees at each stage is\n",
    "\n",
    "1. Compute the **pseudo-residuals**, the derivative of the loss function with respect to the previous model $f_{m-1}$. The equation for the pseudo-residuals for a model at stage $m$ is\n",
    "$$ r_{jm} = - \\left[\\frac{\\partial L(y_j, f(x_j))}{\\partial f} \\right]_{f(x_j) = f_{m-1}(x_j)}. $$\n",
    "1. Train $h_m$ on the **pseudo-residuals** $r_{jm}$.\n",
    "1. Choose $\\gamma_m$ that minimizes $L(y_j, f_{m-1}(x_j) + \\gamma_m h_m(x_j)).$\n",
    "1. Form the improved model equal to\n",
    "$$ f_m(x_j) = f_{m-1}(x_j) + \\gamma_m h_m(x_j). $$\n",
    "1. Repeat until the model includes all $M$ trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does the name gradient come from in gradient boosting trees? Adding a model is analogous to a single iteration in gradient descent. Gradient descent is a minimization algorithm that updates/improves the current answer by taking a step in the direction of the negative of the gradient of the function that is being minimized. The pseudo-residuals represent the direction of greatest reduction in prediction error. Since $h_m$ is trained on the direction of greatest descent of the loss, it will be an approximation of the improvement required for our model to fit the data more closely. $\\gamma_m$ is the step size we should take in the direction of greatest model improvement. Compare the equation above for an improved model with the equation of gradient descent applied to a function $C(\\beta_i)$;\n",
    "\n",
    "$$\\beta^{updated}_i = \\beta^{current}_i - \\gamma \\left(\\left.\\frac{\\partial C}{\\partial \\beta_i}\\right)\\right|_{\\beta_i=\\beta^{current}_i}.$$\n",
    "\n",
    "The concept of pseudo-residuals allows us to generalize gradient boosting trees for any loss function. In fact, when we use a square loss function like the squared error, the pseudo-residual is directly proportional to the residual, $y_j - f(x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting trees have a similar set of hyperparameters as random forests but with some key additions.\n",
    "\n",
    "|Hyperparameter | Description |\n",
    "|---|---|\n",
    "|learning_rate|Multiplicative factor of the tree's contribution to the model|\n",
    "|subsample|Fraction of the training data to use when fitting the trees|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate ranges form 0 to 1, with the modified equation being\n",
    "\n",
    "$$ f_m(x_j) = f_{m-1}(x_j) + \\nu \\gamma_m h_m(x_j), $$\n",
    "\n",
    "where $\\nu$ is the learning rate. The learning rate and subsampling fraction interact with each other, we will see this in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "1. Fit a Gradient Boosted Tree Classifier, evaluate the f1-score\n",
    "2. Compare against the previous 2 models\n",
    "3. What can you do to improve your result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier\n",
    "The idea behind the `VotingClassifier` is to combine conceptually different machine learning classifiers and use a combination of the results to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.\n",
    "\n",
    "## Majority Class Labels (Majority/Hard Voting)\n",
    "In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.\n",
    "\n",
    "E.g., if the prediction for a given sample is\n",
    "- classifier 1 -> class 1\n",
    "- classifier 2 -> class 1\n",
    "- classifier 3 -> class 2\n",
    "\n",
    "the `VotingClassifier (with voting='hard')` would classify the sample as \"class 1\" based on the majority class label.\n",
    "\n",
    "In the cases of a tie, the `VotingClassifier` will select the class based on the ascending sort order. E.g., in the following scenario:\n",
    "- classifier 1 -> class 2\n",
    "- classifier 2 -> class 1\n",
    "\n",
    "the class label 1 will be assigned to the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:39.754294Z",
     "start_time": "2019-09-30T19:21:38.660494Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = DecisionTreeClassifier(random_state=10)\n",
    "clf2 = RandomForestClassifier(n_estimators=10, random_state=10)\n",
    "clf3 = GradientBoostingClassifier(random_state=10)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('dtc', clf1),\n",
    "                                    ('rfc', clf2), \n",
    "                                    ('gbc', clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], \n",
    "                      ['Decision Tree', 'Random Forest', 'Gradient Boosting', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Average Probabilities (Soft Voting)\n",
    "In contrast to majority voting (hard voting), soft voting returns the class label as `argmax` of the sum of predicted probabilities.\n",
    "\n",
    "Specific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability.\n",
    "\n",
    "To illustrate this with a simple example, let’s assume we have 3 classifiers and a 3-class classification problems where we assign equal weights to all classifiers: w1=1, w2=1, w3=1.\n",
    "\n",
    "The weighted average probabilities for a sample would then be calculated as follows:\n",
    "\n",
    "|classifier|\tclass 1\t|class 2\t|class 3|\n",
    "|---|---|---|---|\n",
    "|classifier 1|\tw1 * 0.2|\tw1 * 0.5|\tw1 * 0.3|\n",
    "|classifier 2|\tw2 * 0.6|\tw2 * 0.3|\tw2 * 0.1|\n",
    "|classifier 3|\tw3 * 0.3|\tw3 * 0.4|\tw3 * 0.3|\n",
    "|weighted average|\t0.37\t|0.4|\t0.23|\n",
    "\n",
    "Here, the predicted class label is 2, since it has the highest average probability.\n",
    "\n",
    "The following example illustrates how the decision regions may change when a soft VotingClassifier.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T19:21:41.196824Z",
     "start_time": "2019-09-30T19:21:39.755892Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "# Training classifiers\n",
    "\n",
    "clf1 = DecisionTreeClassifier(random_state=10,max_depth=3)\n",
    "clf2 = RandomForestClassifier(n_estimators=5, max_depth=3, random_state=10)\n",
    "clf3 = GradientBoostingClassifier(random_state=10)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('dtc', clf1),\n",
    "                                    ('rfc', clf2), \n",
    "                                    ('gbc', clf3)], \n",
    "                        voting='soft', weights=[2, 1, 2])\n",
    "clf1 = clf1.fit(X, y)\n",
    "clf2 = clf2.fit(X, y)\n",
    "clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X, y)\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], \n",
    "                      ['Decision Tree', 'Random Forest', 'Gradient Boosting', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "So we now know how to:\n",
    "- Obtain the optimal hyperparameter for our model using GridSeachCV (hyperparameter tuning)\n",
    "- Modelling techniques:\n",
    "    - *Decision Tree Classifier*\n",
    "    - Random Forest Classifiers, \n",
    "    - Extremely Randomized Trees,\n",
    "    - Gradient Boosted Tree Classifiers,\n",
    "    - **Voting Classifier** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
